{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 1 : premier tests de BM25 et BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\guill\\anaconda3\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: rank_bm25 in c:\\users\\guill\\anaconda3\\lib\\site-packages (0.2.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\guill\\anaconda3\\lib\\site-packages (from rank_bm25) (1.24.3)\n",
      "Requirement already satisfied: transformers in c:\\users\\guill\\anaconda3\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Invalid requirement: '#'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\guill\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\guill\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\guill\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\guill\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\guill\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install PyPDF2\n",
    "! pip install rank_bm25\n",
    "! pip install transformers\n",
    "! pip install torch \n",
    "! pip install nltk\n",
    "! pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def bm25_rank(query, documents):\n",
    "    # Tokenisation des documents\n",
    "    tokenized_docs = [word_tokenize(doc.lower(), language='french') for doc in documents]\n",
    "    \n",
    "    # Initialisation de BM25\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Tokenisation de la requête\n",
    "    tokenized_query = word_tokenize(query.lower(), language='french')\n",
    "    \n",
    "    # Calcul des scores BM25\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Création d'une liste de tuples (indice du document, score)\n",
    "    doc_scores = list(enumerate(scores))\n",
    "    \n",
    "    # Tri des documents en fonction des scores (du plus élevé au plus faible)\n",
    "    ranked_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Récupération du meilleur document\n",
    "    best_doc_index = ranked_docs[0][0]\n",
    "    best_doc_score = ranked_docs[0][1]\n",
    "    \n",
    "    return ranked_docs, best_doc_index, best_doc_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def bert_rank(query, documents):\n",
    "    # Chargement du modèle et du tokenizer pré-entraînés\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "    \n",
    "    # Fonction pour obtenir les embeddings\n",
    "    def get_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:,0,:].detach().numpy()\n",
    "        return embeddings[0]\n",
    "    \n",
    "    # Calcul des embeddings des documents\n",
    "    doc_embeddings = np.array([get_embedding(doc) for doc in documents])\n",
    "    \n",
    "    # Calcul de l'embedding de la requête\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calcul de la similarité cosinus\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    # Création d'une liste de tuples (indice du document, similarité)\n",
    "    doc_similarities = list(enumerate(similarities))\n",
    "    \n",
    "    # Tri des documents en fonction des similarités (du plus élevé au plus faible)\n",
    "    ranked_docs = sorted(doc_similarities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Récupération du meilleur document\n",
    "    best_doc_index = ranked_docs[0][0]\n",
    "    best_doc_similarity = ranked_docs[0][1]\n",
    "    \n",
    "    return ranked_docs, best_doc_index, best_doc_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    pdf_paths = [\n",
    "        r'C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf',\n",
    "        r'C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf'\n",
    "    ]\n",
    "    \n",
    "    documents = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]\n",
    "\n",
    "    query = input(\"Entrez votre requête : \")\n",
    "\n",
    "    print(\"\\nRequête :\", query)\n",
    "    \n",
    "    # Classement avec BM25\n",
    "    bm25_ranked_docs, bm25_best_doc_index, bm25_best_doc_score = bm25_rank(query, documents)\n",
    "    print(\"\\nClassement des documents avec BM25 :\")\n",
    "    for idx, score in bm25_ranked_docs:\n",
    "        print(f\"Document : {pdf_paths[idx]}, Score : {score:.2f}\")\n",
    "    print(f\"\\nMeilleur document selon BM25 : {pdf_paths[bm25_best_doc_index]}, Score : {bm25_best_doc_score:.2f}\")\n",
    "    \n",
    "    # Classement avec BERT\n",
    "    bert_ranked_docs, bert_best_doc_index, bert_best_doc_similarity = bert_rank(query, documents)\n",
    "    print(\"\\nClassement des documents avec BERT :\")\n",
    "    for idx, similarity in bert_ranked_docs:\n",
    "        print(f\"Document : {pdf_paths[idx]}, Similarité : {similarity:.4f}\")\n",
    "    print(f\"\\nMeilleur document selon BERT : {pdf_paths[bert_best_doc_index]}, Similarité : {bert_best_doc_similarity:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requête : Combien d’heures au maximum on peut travailler par semaine ?\n",
      "\n",
      "Classement des documents avec BM25 :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -2.87\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Score : -2.90\n",
      "\n",
      "Meilleur document selon BM25 : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -2.87\n",
      "\n",
      "Classement des documents avec BERT :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Similarité : 0.7807\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Similarité : 0.6591\n",
      "\n",
      "Meilleur document selon BERT : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Similarité : 0.7807\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 2 : Ajout de la fonctionnalité de synonyme pour améliorer BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilisation du modèle BERT en français :\n",
    "Dans la fonction bert_rank, nous avons modifié le modèle pour utiliser un modèle BERT pré-entraîné en français, tel que CamemBERT\n",
    "\n",
    "\n",
    "Les modèles BERT pré-entraînés en anglais (bert-base-uncased) ne sont pas optimaux pour traiter des textes en français.\n",
    "CamemBERT est un modèle BERT adapté au français et donnera de meilleurs résultats sur des textes français.\n",
    "Gestion des synonymes en français \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons le WordNet multilingue via NLTK pour obtenir des synonymes en français.\n",
    "La variable lang='fra' est utilisée pour spécifier que nous souhaitons des synonymes en français.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En ajoutant des synonymes aux documents, nous enrichissons leur contenu sémantique.\n",
    "Cela permet à BM25, qui est basé sur la correspondance exacte de termes, de mieux capturer les relations sémantiques entre la requête et les documents.\n",
    "\n",
    "Certains mots peuvent ne pas avoir de synonymes dans WordNet.\n",
    "Les synonymes peuvent inclure des mots qui sont des stop words ; nous avons filtré les tokens non alphabétiques mais pas les stop words parmi les synonymes pour conserver la diversité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_synonyms_to_documents(documents, n):\n",
    "    # Obtenir les stop words français\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    \n",
    "    # Traiter chaque document\n",
    "    new_documents = []\n",
    "    for doc in documents:\n",
    "        # Tokeniser le document\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        \n",
    "        # Identifier les mots complexes (non stop words)\n",
    "        complex_words = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "        \n",
    "        # Pour chaque mot complexe, trouver des synonymes\n",
    "        synonyms = []\n",
    "        for word in set(complex_words):  # Utiliser set pour éviter les doublons\n",
    "            synsets = wn.synsets(word, lang='fra')\n",
    "            # Collecter les synonymes des synsets\n",
    "            syns = set()\n",
    "            for synset in synsets:\n",
    "                for lemma in synset.lemma_names('fra'):\n",
    "                    if lemma != word:\n",
    "                        syns.add(lemma.replace('_', ' '))\n",
    "            # Ajouter jusqu'à 3 synonymes\n",
    "            syns = list(syns)[:n]\n",
    "            synonyms.extend(syns)\n",
    "        \n",
    "        # Ajouter les synonymes à la fin du document\n",
    "        extended_doc = doc + ' ' + ' '.join(synonyms)\n",
    "        new_documents.append(extended_doc)\n",
    "    return new_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_rank(query, documents):\n",
    "    # Tokenisation des documents\n",
    "    tokenized_docs = [word_tokenize(doc.lower(), language='french') for doc in documents]\n",
    "    \n",
    "    # Initialisation de BM25\n",
    "    bm25 = BM25Okapi(tokenized_docs, k1=1.5, b=0.5)\n",
    "    \n",
    "    # Tokenisation de la requête\n",
    "    tokenized_query = word_tokenize(query.lower(), language='french')\n",
    "    \n",
    "    # Calcul des scores BM25\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Création d'une liste de tuples (indice du document, score)\n",
    "    doc_scores = list(enumerate(scores))\n",
    "    \n",
    "    # Tri des documents en fonction des scores (du plus élevé au plus faible)\n",
    "    ranked_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Récupération du meilleur document\n",
    "    best_doc_index = ranked_docs[0][0]\n",
    "    best_doc_score = ranked_docs[0][1]\n",
    "    \n",
    "    return ranked_docs, best_doc_index, best_doc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_rank(query, documents):\n",
    "    # Chargement du modèle et du tokenizer pré-entraînés\n",
    "    tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "    model = AutoModel.from_pretrained('camembert-base')\n",
    "    \n",
    "    # Fonction pour obtenir les embeddings\n",
    "    def get_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:,0,:].detach().numpy()\n",
    "        return embeddings[0]\n",
    "    \n",
    "    # Calcul des embeddings des documents\n",
    "    doc_embeddings = np.array([get_embedding(doc) for doc in documents])\n",
    "    \n",
    "    # Calcul de l'embedding de la requête\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calcul de la similarité cosinus\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    # Création d'une liste de tuples (indice du document, similarité)\n",
    "    doc_similarities = list(enumerate(similarities))\n",
    "    \n",
    "    # Tri des documents en fonction des similarités (du plus élevé au plus faible)\n",
    "    ranked_docs = sorted(doc_similarities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Récupération du meilleur document\n",
    "    best_doc_index = ranked_docs[0][0]\n",
    "    best_doc_similarity = ranked_docs[0][1]\n",
    "    \n",
    "    return ranked_docs, best_doc_index, best_doc_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_avec_synonyms(n):\n",
    "    # Chemins vers vos documents PDF\n",
    "    pdf_paths = [\n",
    "        r'C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf',\n",
    "        r'C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf'\n",
    "    ]\n",
    "    \n",
    "    # Extraction du texte des PDFs\n",
    "    documents = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]\n",
    "    \n",
    "    # Ajouter les synonymes aux documents pour BM25\n",
    "    documents_with_synonyms = add_synonyms_to_documents(documents,n)\n",
    "    \n",
    "    # Votre requête\n",
    "    query = input(\"Entrez votre requête : \")\n",
    "\n",
    "    print(\"\\nRequête :\", query)\n",
    "    \n",
    "    # Classement avec BM25 (documents avec synonymes)\n",
    "    bm25_ranked_docs, bm25_best_doc_index, bm25_best_doc_score = bm25_rank(query, documents_with_synonyms)\n",
    "    print(\"\\nClassement des documents avec BM25 :\")\n",
    "    for idx, score in bm25_ranked_docs:\n",
    "        print(f\"Document : {pdf_paths[idx]}, Score : {score:.2f}\")\n",
    "    print(f\"\\nMeilleur document selon BM25 : {pdf_paths[bm25_best_doc_index]}, Score : {bm25_best_doc_score:.2f}\")\n",
    "    \n",
    "    # Classement avec BERT (documents originaux)\n",
    "    bert_ranked_docs, bert_best_doc_index, bert_best_doc_similarity = bert_rank(query, documents)\n",
    "    print(\"\\nClassement des documents avec BERT :\")\n",
    "    for idx, similarity in bert_ranked_docs:\n",
    "        print(f\"Document : {pdf_paths[idx]}, Similarité : {similarity:.4f}\")\n",
    "    print(f\"\\nMeilleur document selon BERT : {pdf_paths[bert_best_doc_index]}, Similarité : {bert_best_doc_similarity:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec 3 synonymes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requête : Combien d’heures au maximum on peut travailler par semaine ?\n",
      "\n",
      "Classement des documents avec BM25 :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -3.76\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Score : -3.77\n",
      "\n",
      "Meilleur document selon BM25 : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -3.76\n",
      "\n",
      "Classement des documents avec BERT :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Similarité : 0.8213\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Similarité : 0.8124\n",
      "\n",
      "Meilleur document selon BERT : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Similarité : 0.8213\n"
     ]
    }
   ],
   "source": [
    "main_avec_synonyms(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avec 6 synonyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requête : Combien d’heures au maximum on peut travailler par semaine ?\n",
      "\n",
      "Classement des documents avec BM25 :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -4.22\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Score : -4.23\n",
      "\n",
      "Meilleur document selon BM25 : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -4.22\n",
      "\n",
      "Classement des documents avec BERT :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Similarité : 0.8213\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Similarité : 0.8124\n",
      "\n",
      "Meilleur document selon BERT : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Similarité : 0.8213\n"
     ]
    }
   ],
   "source": [
    "main_avec_synonyms(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST 3 : test avec ajout des synonyme à la query et ajout des synonyme après chaque mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\guill\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classement des documents avec BM25 :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -3.49\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Score : -3.51\n",
      "\n",
      "Meilleur document selon BM25 : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Score : -3.49\n",
      "\n",
      "Classement des documents avec BERT :\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Similarité : 0.8213\n",
      "Document : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf, Similarité : 0.8124\n",
      "\n",
      "Meilleur document selon BERT : C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf, Similarité : 0.8213\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Définir les stop words français\n",
    "stop_words = set(stopwords.words('french'))\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            page = pdf_reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "def get_synonyms(word):\n",
    "    synsets = wn.synsets(word, lang='fra')\n",
    "    syns = set()\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemma_names('fra'):\n",
    "            if lemma != word:\n",
    "                syns.add(lemma.replace('_', ' '))\n",
    "    return list(syns)[:3]\n",
    "\n",
    "def add_synonyms_inline(documents):\n",
    "    # Obtenir les stop words français\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    new_documents = []\n",
    "    for doc in documents:\n",
    "        tokens = word_tokenize(doc, language='french')\n",
    "        new_tokens = []\n",
    "        for word in tokens:\n",
    "            new_tokens.append(word)\n",
    "            if word.lower() not in stop_words and word.isalpha():\n",
    "                synonyms = get_synonyms(word)\n",
    "                new_tokens.extend(synonyms)\n",
    "        new_doc = ' '.join(new_tokens)\n",
    "        new_documents.append(new_doc)\n",
    "    return new_documents\n",
    "\n",
    "\n",
    "def expand_query(query):\n",
    "    tokens = word_tokenize(query, language='french')\n",
    "    expanded_tokens = []\n",
    "    for word in tokens:\n",
    "        expanded_tokens.append(word)\n",
    "        if word.lower() not in stop_words and word.isalpha():\n",
    "            synonyms = get_synonyms(word)\n",
    "            expanded_tokens.extend(synonyms)\n",
    "    return ' '.join(expanded_tokens)\n",
    "\n",
    "def bm25_rank(query, documents):\n",
    "    # Tokenisation des documents\n",
    "    tokenized_docs = [word_tokenize(doc.lower(), language='french') for doc in documents]\n",
    "    \n",
    "    # Initialisation de BM25 avec des paramètres ajustés (optionnel)\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    \n",
    "    # Tokenisation de la requête\n",
    "    tokenized_query = word_tokenize(query.lower(), language='french')\n",
    "    \n",
    "    # Calcul des scores BM25\n",
    "    scores = bm25.get_scores(tokenized_query)\n",
    "    \n",
    "    # Création d'une liste de tuples (indice du document, score)\n",
    "    doc_scores = list(enumerate(scores))\n",
    "    \n",
    "    # Tri des documents en fonction des scores (du plus élevé au plus faible)\n",
    "    ranked_docs = sorted(doc_scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Récupération du meilleur document\n",
    "    best_doc_index = ranked_docs[0][0]\n",
    "    best_doc_score = ranked_docs[0][1]\n",
    "    \n",
    "    return ranked_docs, best_doc_index, best_doc_score\n",
    "\n",
    "def bert_rank(query, documents):\n",
    "    # Chargement du modèle et du tokenizer pré-entraînés pour le français\n",
    "    tokenizer = AutoTokenizer.from_pretrained('camembert-base')\n",
    "    model = AutoModel.from_pretrained('camembert-base')\n",
    "    \n",
    "    # Fonction pour obtenir les embeddings\n",
    "    def get_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:,0,:].detach().numpy()\n",
    "        return embeddings[0]\n",
    "    \n",
    "    # Calcul des embeddings des documents\n",
    "    doc_embeddings = np.array([get_embedding(doc) for doc in documents])\n",
    "    \n",
    "    # Calcul de l'embedding de la requête\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    # Calcul de la similarité cosinus\n",
    "    similarities = cosine_similarity([query_embedding], doc_embeddings)[0]\n",
    "    \n",
    "    # Création d'une liste de tuples (indice du document, similarité)\n",
    "    doc_similarities = list(enumerate(similarities))\n",
    "    \n",
    "    # Tri des documents en fonction des similarités (du plus élevé au plus faible)\n",
    "    ranked_docs = sorted(doc_similarities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Récupération du meilleur document\n",
    "    best_doc_index = ranked_docs[0][0]\n",
    "    best_doc_similarity = ranked_docs[0][1]\n",
    "    \n",
    "    return ranked_docs, best_doc_index, best_doc_similarity\n",
    "\n",
    "def main():\n",
    "    # Chemins vers vos documents PDF\n",
    "    pdf_paths = [\n",
    "        r'C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION PAIE MAJ 09-24.pdf',\n",
    "        r'C:\\Users\\guill\\OneDrive\\Documents\\ESILV\\A4\\Parcours recherche\\Sujet\\Doc_pdf\\FORMATION SOCIALE MAJ 09-24.pdf'\n",
    "    ]\n",
    "    \n",
    "    # Extraction du texte des PDFs\n",
    "    documents = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_paths]\n",
    "\n",
    "    documents_synonyms = add_synonyms_inline(documents)\n",
    "    \n",
    "    # Votre requête\n",
    "    query = input(\"Entrez votre requête : \")\n",
    "    \n",
    "    # Étendre la requête avec des synonymes\n",
    "    expanded_query = expand_query(query)\n",
    "    \n",
    "    # Classement avec BM25 (documents originaux, requête étendue)\n",
    "    bm25_ranked_docs, bm25_best_doc_index, bm25_best_doc_score = bm25_rank(expanded_query, documents)\n",
    "    print(\"Classement des documents avec BM25 :\")\n",
    "    for idx, score in bm25_ranked_docs:\n",
    "        print(f\"Document : {pdf_paths[idx]}, Score : {score:.2f}\")\n",
    "    print(f\"\\nMeilleur document selon BM25 : {pdf_paths[bm25_best_doc_index]}, Score : {bm25_best_doc_score:.2f}\")\n",
    "    \n",
    "    # Classement avec BERT (documents originaux)\n",
    "    bert_ranked_docs, bert_best_doc_index, bert_best_doc_similarity = bert_rank(query, documents)\n",
    "    print(\"\\nClassement des documents avec BERT :\")\n",
    "    for idx, similarity in bert_ranked_docs:\n",
    "        print(f\"Document : {pdf_paths[idx]}, Similarité : {similarity:.4f}\")\n",
    "    print(f\"\\nMeilleur document selon BERT : {pdf_paths[bert_best_doc_index]}, Similarité : {bert_best_doc_similarity:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test avec ajout des synonyme à la fin du doc : moins bon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test avec synonyme après chaque mots : moins bon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test avec synonyme dans la requete : moins bon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test avec synonyme dans requete + après chaque mots : vraiment moins bon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
